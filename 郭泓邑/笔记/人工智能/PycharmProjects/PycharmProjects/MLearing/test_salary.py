import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# è¡¥å……çŸ¥è¯†ç‚¹ï¼š
# çº¿æ€§å›å½’æ¨¡å‹ï¼šå› å˜é‡yæ˜¯è¿ç»­å€¼é¢„æµ‹çš„é—®é¢˜ï¼Œç»“æœæ˜¯ä¸€ä¸ªå…·ä½“çš„ æ•°å€¼
# çº¿æ€§å›é¡¾æ¨¡å‹åœºæ™¯ï¼šé¢„æµ‹è–ªèµ„ï¼Œé¢„æµ‹è¯„åˆ†ï¼Œé¢„æµ‹æˆ¿ä»·ç­‰å…·ä½“æ•°å€¼é—®é¢˜

# æ¢¯åº¦ä¸‹é™æ³•è¡¥å……çŸ¥è¯†ç‚¹ï¼š
# SGDRegressor: ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™çš„çº¿æ€§å›å½’æ¨¡å‹
# ä¼˜ç‚¹ï¼šé€‚åˆå¤§æ•°æ®é›†ï¼Œæ”¯æŒåœ¨çº¿å­¦ä¹ ï¼Œå†…å­˜æ•ˆç‡é«˜
# ç¼ºç‚¹ï¼šéœ€è¦è°ƒå‚ï¼Œå¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿ

# è¯»å–æ•°æ®æ–‡ä»¶
df = pd.read_csv("salary_data.csv")

# åˆ é™¤ç›®æ ‡å˜é‡ é¢„æµ‹å°±æ˜¯è–ªèµ„ï¼ˆç›®æ ‡å˜é‡ï¼‰
data = df.drop("æœˆå‡è–ªèµ„", axis=1)
target = df["æœˆå‡è–ªèµ„"]  # æ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼

# ============= ç‰¹å¾å·¥ç¨‹ =============
# å®šä¹‰ç‰¹å¾ç±»å‹
æ•°å€¼ç±»å‹ç‰¹å¾åˆ— = ["å·¥ä½œå¹´é™", "ç»©æ•ˆè¯„åˆ†"]
åˆ†ç±»ç±»å‹ç‰¹å¾åˆ— = ["èŒä½ç­‰çº§", "å­¦å†æ°´å¹³"]

# æ„å»ºé¢„å¤„ç†ç®¡é“ åˆ—è½¬æ¢å™¨å¤„ç†æ•°æ®çš„ç‰¹å¾ï¼Œç­‰æ•°æ®æ¥æ—¶è‡ªåŠ¨å¤„ç†
# å¯¹æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–ï¼šå·¥ä½œå¹´é™ã€ç»©æ•ˆè¯„åˆ†ç­‰æ•°å€¼ç‰¹å¾,ä½¿ç”¨StandardScaler()å°†å…¶è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„æ ‡å‡†åˆ† # å¸ƒã€‚æ¢¯åº¦ä¸‹é™å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿï¼Œæ ‡å‡†åŒ–èƒ½åŠ é€Ÿæ”¶æ•›
# å¯¹åˆ†ç±»ç‰¹å¾ç‹¬çƒ­ç¼–ç ï¼šèŒä½ç­‰çº§ã€å­¦å†æ°´å¹³ç­‰åˆ†ç±»ç‰¹å¾ä½¿ç”¨OneHotEncoder()è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹ä¸èƒ½ç›´æ¥å¤„ç†æ–‡æœ¬ç±»åˆ«
prePip = ColumnTransformer(
    transformers=[
        # å‡ ç±»ç‰¹å¾ï¼Œç»„è£…å‡ ä¸ªç®¡é“
        # å¤„ç†æ•°å€¼ç±»å‹ç‰¹å¾çš„ç®¡é“
        ("num", StandardScaler(), æ•°å€¼ç±»å‹ç‰¹å¾åˆ—),  # ä¸‰å…ƒç»„
        # å¤„ç†ç±»åˆ«ç±»å‹ç‰¹å¾çš„ç®¡é“
        ("cat", OneHotEncoder(handle_unknown="ignore"), åˆ†ç±»ç±»å‹ç‰¹å¾åˆ—)
    ]
)

# æ„å»ºæ¢¯åº¦ä¸‹é™æ¨¡å‹ - ä½¿ç”¨SGDRegressoræ›¿ä»£LinearRegression
model = Pipeline(
    steps=[
        # ç‰¹å¾å¤„ç†çš„ç®¡é“
        ("prePip", prePip),
        # æ¢¯åº¦ä¸‹é™å›å½’æ¨¡å‹ - æ›¿ä»£çº¿æ€§å›å½’
        ("regressor", SGDRegressor(
            loss='squared_error',  # ä½¿ç”¨å¹³æ–¹æŸå¤±ï¼Œç­‰åŒäºæ™®é€šçº¿æ€§å›å½’
            penalty='l2',  # L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            alpha=0.0001,  # æ­£åˆ™åŒ–å¼ºåº¦
            learning_rate='invscaling',  # å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
            eta0=0.01,  # åˆå§‹å­¦ä¹ ç‡
            max_iter=1000,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
            tol=1e-3,  # æ”¶æ•›å®¹å¿åº¦
            random_state=42,  # éšæœºç§å­
            early_stopping=False,  # ä¸ä½¿ç”¨æ—©åœ
            validation_fraction=0.1,  # éªŒè¯é›†æ¯”ä¾‹ï¼ˆå¦‚æœä½¿ç”¨æ—©åœï¼‰
            n_iter_no_change=5  # æ— æ”¹è¿›è¿­ä»£æ¬¡æ•°ï¼ˆå¦‚æœä½¿ç”¨æ—©åœï¼‰
        ))
    ]
)

# åˆ’åˆ†æ•°æ®é›†
(X_train,  # 80%è®­ç»ƒé›†
 X_test,  # 20%æµ‹è¯•é›†
 y_train,  # 80%è®­ç»ƒé›†å¯¹åº”çš„ç›®æ ‡å˜é‡
 y_test  # 20%æµ‹è¯•é›†å¯¹åº”çš„ç›®æ ‡å˜é‡
 ) = train_test_split(
    data, target, train_size=0.8,
    random_state=42
)

print("=" * 60)
print("ğŸ¯ æ¢¯åº¦ä¸‹é™å›å½’æ¨¡å‹è®­ç»ƒ")
print("=" * 60)
print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
print(f"ç‰¹å¾æ•°é‡: {X_train.shape[1]}")

# è®­ç»ƒæ¨¡å‹
print("\nå¼€å§‹è®­ç»ƒæ¢¯åº¦ä¸‹é™æ¨¡å‹...")
model.fit(X_train, y_train)

# è·å–è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if hasattr(model.named_steps['regressor'], 'n_iter_'):
    print(f"å®é™…è¿­ä»£æ¬¡æ•°: {model.named_steps['regressor'].n_iter_}")

# é¢„æµ‹
y_pred = model.predict(X_test)

# æ¨¡å‹è¯„ä¼°
print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
print("-" * 40)

# è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.2f}")
print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.2f}")
print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.2f}")
print(f"å†³å®šç³»æ•° (RÂ²): {r2:.4f}")

# è®¡ç®—å¹³å‡è¯¯å·®ç™¾åˆ†æ¯”
mean_salary = np.mean(y_test)
error_percentage = (mae / mean_salary) * 100
print(f"å¹³å‡ç»å¯¹è¯¯å·®å å¹³å‡è–ªèµ„çš„: {error_percentage:.2f}%")

# è·å–æ¨¡å‹ç³»æ•°ä¿¡æ¯
regressor = model.named_steps['regressor']
print(f"\nğŸ” æ¨¡å‹å‚æ•°ä¿¡æ¯:")
print(f"æœ€ç»ˆå­¦ä¹ ç‡: {regressor.eta0}")  # æ³¨æ„ï¼šå¯¹äºinvscalingï¼Œè¿™æ˜¯åˆå§‹å­¦ä¹ ç‡
print(f"ä½¿ç”¨çš„æŸå¤±å‡½æ•°: {regressor.loss}")
print(f"æ­£åˆ™åŒ–ç±»å‹: {regressor.penalty}")
print(f"æ­£åˆ™åŒ–å¼ºåº¦: {regressor.alpha}")

# ä¿å­˜æ¨¡å‹
joblib.dump(model, "sgd_model.pkl")
print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: sgd_model.pkl")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
# è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
plt.rcParams['axes.unicode_minus'] = False


# ç»˜åˆ¶é¢„æµ‹ç»“æœ
def plot_sgd_regression(y_test, y_pred, model_name="æ¢¯åº¦ä¸‹é™å›å½’"):
    """ç»˜åˆ¶æ¢¯åº¦ä¸‹é™å›å½’çš„é¢„æµ‹ç»“æœ"""

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # 1. é¢„æµ‹ vs å®é™…å€¼æ•£ç‚¹å›¾
    ax1.scatter(y_test, y_pred, alpha=0.6, color='blue')
    # å®Œç¾é¢„æµ‹çº¿
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
    ax1.set_xlabel('å®é™…è–ªèµ„')
    ax1.set_ylabel('é¢„æµ‹è–ªèµ„')
    ax1.set_title(f'{model_name} - é¢„æµ‹æ•ˆæœå¯¹æ¯”\nRÂ² = {r2:.4f}')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. æ®‹å·®å›¾
    residuals = y_test - y_pred
    ax2.scatter(y_pred, residuals, alpha=0.6, color='green')
    ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)
    ax2.set_xlabel('é¢„æµ‹è–ªèµ„')
    ax2.set_ylabel('æ®‹å·® (å®é™…-é¢„æµ‹)')
    ax2.set_title('æ®‹å·®åˆ†æå›¾')
    ax2.grid(True, alpha=0.3)

    # 3. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
    ax3.hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')
    ax3.axvline(x=0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®çº¿')
    ax3.set_xlabel('é¢„æµ‹è¯¯å·®')
    ax3.set_ylabel('é¢‘æ•°')
    ax3.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. å®é™…å€¼ vs é¢„æµ‹å€¼æŠ˜çº¿å›¾ï¼ˆæŒ‰æ’åºåçš„ç´¢å¼•ï¼‰
    sorted_indices = np.argsort(y_test)
    y_test_sorted = y_test.iloc[sorted_indices].values
    y_pred_sorted = y_pred[sorted_indices]

    ax4.plot(range(len(y_test_sorted)), y_test_sorted, 'b-', label='å®é™…è–ªèµ„', alpha=0.7)
    ax4.plot(range(len(y_pred_sorted)), y_pred_sorted, 'r--', label='é¢„æµ‹è–ªèµ„', alpha=0.7)
    ax4.set_xlabel('æ ·æœ¬åºå· (æŒ‰å®é™…è–ªèµ„æ’åº)')
    ax4.set_ylabel('è–ªèµ„')
    ax4.set_title('å®é™…è–ªèµ„ vs é¢„æµ‹è–ªèµ„è¶‹åŠ¿')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("æ¢¯åº¦ä¸‹é™_è–ªèµ„é¢„æµ‹æ•ˆæœåˆ†æ.png", dpi=300, bbox_inches='tight')
    plt.show()


# è°ƒç”¨å‡½æ•°ç»˜åˆ¶æ¢¯åº¦ä¸‹é™å›å½’çš„ç»“æœ
plot_sgd_regression(y_test, y_pred)


# æ¨¡å‹è°ƒä¼˜å»ºè®®å‡½æ•°
def æ¨¡å‹è°ƒä¼˜å»ºè®®(å½“å‰r2, y_test, y_pred):
    """æä¾›æ¨¡å‹è°ƒä¼˜å»ºè®®"""

    print("\n" + "=" * 60)
    print("ğŸ”§ æ¨¡å‹è°ƒä¼˜å»ºè®®")
    print("=" * 60)

    # åˆ†æå½“å‰æ€§èƒ½
    if r2 > 0.8:
        print("âœ… å½“å‰æ¨¡å‹æ€§èƒ½ä¼˜ç§€ (RÂ² > 0.8)")
    elif r2 > 0.6:
        print("ğŸ“ˆ å½“å‰æ¨¡å‹æ€§èƒ½è‰¯å¥½ (RÂ² > 0.6)")
    elif r2 > 0.4:
        print("âš ï¸  å½“å‰æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ (RÂ² > 0.4)ï¼Œå»ºè®®è°ƒä¼˜")
    else:
        print("âŒ å½“å‰æ¨¡å‹æ€§èƒ½è¾ƒå·® (RÂ² â‰¤ 0.4)ï¼Œéœ€è¦è°ƒä¼˜")

    # è®¡ç®—è¯¯å·®ç»Ÿè®¡
    residuals = y_test - y_pred
    residual_std = np.std(residuals)

    print(f"\nğŸ“Š è¯¯å·®åˆ†æ:")
    print(f"æ®‹å·®æ ‡å‡†å·®: {residual_std:.2f}")
    print(f"æœ€å¤§æ­£è¯¯å·®: {residuals.max():.2f}")
    print(f"æœ€å¤§è´Ÿè¯¯å·®: {residuals.min():.2f}")

    # è°ƒä¼˜å»ºè®®
    print(f"\nğŸ’¡ æ¢¯åº¦ä¸‹é™è°ƒä¼˜å»ºè®®:")

    if residual_std > np.std(y_test) * 0.5:
        print("1. å°è¯•å‡å°å­¦ä¹ ç‡ (eta0=0.001) ä»¥è·å¾—æ›´ç¨³å®šçš„æ”¶æ•›")
    else:
        print("1. å½“å‰å­¦ä¹ ç‡è®¾ç½®åˆç†")

    if r2 < 0.6:
        print("2. å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•° (max_iter=2000)")
        print("3. å°è¯•ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥: 'constant' æˆ– 'adaptive'")
        print("4. è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦ alpha (å°è¯• 0.001 æˆ– 0.00001)")

    print("5. è€ƒè™‘æ·»åŠ å¤šé¡¹å¼ç‰¹å¾æˆ–äº¤äº’é¡¹")
    print("6. æ£€æŸ¥ç‰¹å¾å·¥ç¨‹ï¼Œå¯èƒ½éœ€è¦æ›´å¤šç›¸å…³ç‰¹å¾")


# è°ƒç”¨è°ƒä¼˜å»ºè®®å‡½æ•°
æ¨¡å‹è°ƒä¼˜å»ºè®®(r2, y_test, y_pred)

print("\n" + "=" * 60)
print("ğŸ‰ æ¢¯åº¦ä¸‹é™å›å½’åˆ†æå®Œæˆ!")
print("=" * 60)
print("æ€»ç»“:")
print(f"- ä½¿ç”¨SGDRegressoræ›¿ä»£LinearRegression")
print(f"- æ¨¡å‹RÂ²å¾—åˆ†: {r2:.4f}")
print(f"- æ¨¡å‹å·²ä¿å­˜: sgd_model.pkl")
print(f"- å¯è§†åŒ–ç»“æœå·²ä¿å­˜: æ¢¯åº¦ä¸‹é™_è–ªèµ„é¢„æµ‹æ•ˆæœåˆ†æ.png")
print("\nä¸‹ä¸€æ­¥å»ºè®®:")
print("1. æ ¹æ®è°ƒä¼˜å»ºè®®è°ƒæ•´è¶…å‚æ•°")
print("2. å°è¯•ä¸åŒçš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•")
print("3. è€ƒè™‘ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³å‚æ•°")