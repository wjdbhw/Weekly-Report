 HDFS的数据读写流程?
HDFS数据写入流程：
客户端向NameNode发送写请求
, 这一步, 客户端会将文件名、文件大小告诉
NameNode. 然后NameNode会记录这些信息, 并验证客户端的权限, 如果通过
才进行下一步.
NameNode通过查找集群的信息, 将能够存储数据的DataNode以及存储位置返
回给客户端.
客户端会请求相应的DataNode写入数据, 数据按块写入磁盘, 之后DataNode会
将写入的数据块复制给其他DataNode已完成数据冗余操作. DataNode会向
NameNode汇
报更新的数据信息, NameNode更新元数据.
HDFS数据读取流程：
客户端向NameNode发送读请求
, NameNode会查看存储在内存中的元数据信
息
NNameNode向客户端返回目标数据的元数据信息(包含目标数据的数据块名称
及所在DataNode)
客户端向DataNode请求读取数据块的数据.
DataNode以数据流的形式将数据发送给客户端.
2. HBase的数据读写流程?
HBase数据写入流程：
客户端首先访问Zookeeper，获取元数据所在的RegionServer地址
客户端访问元数据，根据RowKey查找目标数据所在的Region和RegionServer
客户端向目标RegionServer发送写请求
RegionServer接收写请求后，先将数据写入WAL（Write-Ahead Log）预写日
志，保证数据不丢失
然后将数据写入MemStore（内存缓存）
向客户端返回写入成功的响应
当MemStore达到一定大小后，会触发Flush操作，将数据持久化到HDFS上的
StoreFile（HFile）
HBase数据读取流程：
客户端首先访问Zookeeper，获取元数据所在的RegionServer地址
客户端访问元数据，根据RowKey查找目标数据所在的Region和RegionServer
客户端向目标RegionServer发送读请求
RegionServer接收请求后，按以下顺序查找数据：
A) 首先在缓存块中
查找
B) 如果缓存块中没有，在MemStore（写缓存）中
查找
C) 如果MemStore中也没有，在StoreFile（HFile）中
查找
合并所有找到的数据，返回最新版本的数据给客户端
3. Elasticsearch的数据读写流程?
Elasticsearch数据写入流程： 当向集群写入数据时, 首先会将数据写入至主分片
中,
待主分片写入完成后才会将数据写入至其他的副本分片.
客户端向集群的任意一个节点发送写入请求
. 例如节点3
节点3根据路由算法确定要写入的文档所在的分片, 例如P1分区, 节点3将请求
转发给P1分区所在的节点1
节点1在P1主分片上执行操作, 如果数据写入成功,
则将请求并发的转发给P1所
有的副本, 副本分片写入成功, 会向节点1返回写入成功, 当所有副本写入成功
后, 由节点3向客户端返回写入成功的信息.
Elasticsearch数据读取流程：
在读取文档数据时, 为了负载均衡, 文档可以在主分片或者任意的副本分片中读取,
如果读取请求有多个, 那么这些请求会被
均匀的分配到所有分片中.
客户端向任意一个节点(如节点3)发送读请求;
节点3根据指定文档的id判断文档存储在P1分片中, 于是将请求
所在的节点, 即节点1(也可以是R1副本分片, 对应的节点就是节点2).
转发给P1分片
节点1在P1主分片上执行读取数据操作, 操作完成后将数据返回给节点3, 节点3
将数据返回给客户端.
4. Zookeeper的监听机制的运行流程?
客户端向Zookeeper服务
器注册监听（Watcher） ，指定要监听的节点路径
Zookeeper服务
器将监听信息存储在WatchManager中
当
被
监听的节点发生变化时（如数据更新、节点删除、子节点变化）
Zookeeper服务
器触发相应的事件（Event）
服务
器向注册了该
监听的客户端发送事件通知
客户端接收到通知后，执行相应的回调处理逻辑
监听是一次性
的，触发后自动
失效，如需继续监听需要重新注册
5. MapReduce的工作流程?
Input阶段：读取HDFS上的输入数据，将数据分割成多个Split，每个Split对应一个
Map任务
Map阶段：每个Map任务并行处理一个Split，将输入数据转换为键值对<key,
value>，输出到本地磁盘
Shuffle阶段：
A) Partition：根据key将Map输出分配到不同Reducer
B) Sort：对每个Partition内的数据按key排序
C) Copy：将Map输出通过网络传输到Reducer
D) Merge：Reducer合并来自不同Mapper的数据
Reduce阶段：每个Reduce任务
处理一组相同key的数据，执行reduce函数，输出最
终结果
Output阶段：将Reduce的输出结果写入HDFS
6. YARN的工作流程?
客户端向ResourceManager提交应用程序（Application）
ResourceManager为应用程序分配第一个Container，并在NodeManager上启
动ApplicationMaster
ApplicationMaster启动后，向ResourceManager注册自己
ApplicationMaster根据任务需求，向ResourceManager申请资源（Container）
ResourceManager根据资源情况，分配Container给ApplicationMaster
ApplicationMaster获得Container后，与对应的NodeManager通信，启动任务
各NodeManager上的任务
执行，并向ApplicationMaster汇
报进度
任务
执行完成后，ApplicationMaster向ResourceManager注销并释放资源
7. Kafka的数据存储机制?
Topic和Partition结构：
Topic
被分为多个Partition，每个Partition是一个有序的消息序列
Partition分布在不同Broker上，实现负载均衡
存储：
每个Partition对应一个日志文件，消息顺序追加写入
消息持久化到磁盘，不会因为
消
费而删除
被
每条消息有唯一的offset（偏移量）标识
Segment分段：
Partition
被分为多个Segment（分段）
每个Segment包含：.log（数据文件） 、.index（索引文件）
达到一定大小或时间后创建新Segment
消息保留策略：
按时间保留：超过
保留时间的消息
删除
被
按大小保留：超过存储大小限制后删除旧消息
副本机制：
每个Partition有多个副本（Replica）
分为Leader副本和Follower副本，保证数据可靠
性
8. Sqoop的数据导入导出过程?
Sqoop数据导入（从关系型数据库到HDFS） ：
用户执行Sqoop import命令，指定数据库连接信息和目标路径
Sqoop连接到关系型数据库，获取表的元数据信息
Sqoop根据表的主键或指定字段，将数据分割成多个Split
Sqoop启动多个Map任务（没有Reduce任务） ，每个Map任务
处理一个Split
每个Map任务通过JDBC连接数据库，读取对应的数据分片
Map任务将读取的数据写入HDFS的目标路径
Sqoop数据导出（从HDFS到关系型数据库） ：
用户执行Sqoop export命令，指定HDFS数据路径和目标数据库表
Sqoop读取HDFS上的数据文件，将其分割成多个Split
Sqoop启动多个Map任务，每个Map任务
处理一个Split
每个Map任务通过JDBC连接数据库，将数据批量插入到目标表中
所有Map任务完成后，数据导出完成
9. Spark和Hive的异同点?
相同点：
都基于Hadoop生态：Spark和Hive都可以运行在Hadoop集群上，使用HDFS作
为底层存储。
都支持SQL查询
类SQL语言进行数据查询
：Spark提供Spark SQL，Hive提供HiveQL，都允许
和分析。
用
户使用
都适合大数据处理：两者都能处理TB级别以上的数据集，支持分布式计算。
不同点：
执行引擎：Hive默认将SQL转换为MapReduce任务
存的RDD计算模型，执行速度更快。
执行，而Spark使用基于内
处理速度：Spark基于内存计算，执行速度比Hive更快；Hive需要频繁读写磁
盘，延迟较高。
应用场景：Hive主要用于离线批处理和数据仓库，不适合实时查询；Spark支
持批处理、流处理、机器学习等多种场景。
学习成本：Hive的HiveQL与SQL非常相似，学习成本低；Spark需要掌握
Scala/Python/Java编程，学习曲线相对陡峭。
10. Hive和MapReduce的区别?
编程语言：Hive使用类SQL的HiveQL语言，MapReduce使用Java/Python等编
程语言
开发难度：Hive学习成本低，适合SQL熟悉的用
的程序代码，开发难度高
户；MapReduce需要编写完整
执行方式：Hive将HQL自动
动编写Map和Reduce逻辑
转换为MapReduce任务
执行；MapReduce需要手
应用场景：Hive适合数据仓库和批量数据分析；MapReduce适合需要精细控制
的复杂数据处理
灵活性
：Hive功能相对固定，MapReduce可以实现任意复杂的处理逻辑
执行效率：Hive有一定的转换开销，MapReduce直接执行效率更高
11. Flink和Spark的异同点?
Spark介绍：
Spark是一个快速、通用的大数据处理引擎，基于内存计算，支持批处理和流处
理。
核
心特点：
A) RDD：弹性分布式数据集，是Spark的核
心抽象
B) 内存计算：数据缓存在内存中，处理速度快
C) 多种API：支持Scala、Java、Python、R等多种语言
D) 统一引擎：支持批处理（Spark Core） 、流处理（Spark Streaming） 、机器
学习（MLlib） 、图计算（GraphX）
Flink介绍：
Flink是一个分布式流处理框架，真正的流式处理引擎，也支持批处理。
核
心特点：
A) 流优先：将批处理视为流处理的特例
B) 低延迟：毫秒级延迟，适合实时处理
C) 状态管理：提供强大的状态管理和容错机制
D) 事件时间：支持事件时间和水位线（Watermark）机制
Spark和Flink的区别：
A) 处理模型：Spark基于微批处理，Flink是真正的流式处理
B) 延迟：Spark延迟较高（秒级） ，Flink延迟更低（毫秒级）
C) 适用场景：Spark更适合批处理和准实时场景，Flink更适合实时流处理和复杂事
件处理
D) 状态管理：Flink的状态管理和容错机制更强大，支持精确一次（Exactly-once）
语义
12. Flume和Kafka的异同点?
Flume介绍（3分） ：
Flume是一个分布式、可靠、高可用的日志采集系统，主要用于收集、聚合和
传输大量日志数据。
核
心组件：
A) Source：数据源，负责接收数据
B) Channel：缓冲通道，临时存储数据
C) Sink：目的地，负责将数据传输到目标系统
特点：专注于日志采集，采用
推
送模式（Push） ，数据流向单一。
Kafka介绍：
Kafka是一个分布式消息队列系统，采用发布-订阅模式，用于构建实时数据管
道
和流式应用。
核
心组件：
A) Producer：消息生产者
B) Broker：消息服务
器，存储消息
C) Consumer：消息消
费者
D) Topic：消息主题，消息分类
特点：高吞吐量、持久化存储、支持多消
费者，采用拉取模式（Pull） 。
Flume和Kafka的区别（4分） ：
A) 定位不同：Flume专
注于日志采集，Kafka是通用
消息队列系统
B) 数据模式：Flume采用
送模式，Kafka采用拉取模式
推
C) 消
费方式：Flume数据
消
被
费后即删除，Kafka支持消息持久化和重复消
费
D) 应用场景：Flume适合日志收集并直接写入HDFS等存储系统，Kafka适合作
为数据缓冲和多个下游系统
的数据源
13. Flume 和 Sqoop 的异同点？
相同点：
都是大数据采集工具，用于数据的传输和迁移
都可以将数据导入到HDFS等存储系统
都支持分布式部署，具有高可用
性
不同点：
数据源：Flume主要采集流式日志数据（如服务
要用于关系型数据库和HDFS之间的数据传输
器日志、应用日志） ；Sqoop主
数据类型：Flume处理实时流式数据；Sqoop处理批量结构化数据
工作方式：Flume采用事件驱动模式，实时采集；Sqoop采用批处理模式，定
期执行
应用场景：Flume适合日志收集和实时数据采集；Sqoop适合数据库数据的批
量导入导出
14. Hive的Driver如何将HQL转换为MapReduce任务?
解
析器（SQL Parser） ：将SQL字符串
转换成抽象语法树AST，这一步一般都
用第三方工具库完成；对AST进行语法分析，比如表是否存在、字段是否存
在、SQL语义是否有误
编译器（Physical Plan） ：将AST编译生成逻辑执行计划
优化器（Query Optimizer） ：对逻辑执行计划进行优化
执行器（Execution） ：把逻辑执行计划转换成可以运行的物理计划。对于Hive
来说，就是MapReduce任务
15. Zookeeper 实现分布式锁的核
心流程？
客户端在Zookeeper上创建一个临时顺序节点（Ephemeral Sequential Node）
客户端获取锁目录下的所有子节点列表
客户端判断自己创建的节点是否是序号最小的节点
如果是最小节点，则获取锁成功，执行业务逻辑
如果不是最小节点，则对前一个节点设置监听（Watcher）
当前一个节点
被
删除时，收到通知，重新判断自己是否是最小节点
业务逻辑执行完成后，删除自己创建的节点，释放锁
如果客户端断开连接，临时节点自动删除，自动释放锁
16. HBase 的 Region 分裂机制及运行流程？
触发条件：
Region大小超过配置的阈值（默认10GB）
Region中
某个Store的大小超过阈值
分裂流程：
RegionServer检测到Region需要分裂，暂停对该Region的写操作
RegionServer在HDFS上创建一个splits目录，准备分裂
RegionServer将Region在中间位置（RowKey）切分成两个子Region
RegionServer在Meta表中更新Region信息，标记父Region为offline状态
RegionServer创建两个子Region的引用文件，指向父Region的数据文件
RegionServer将两个子Region上线，开始对外提供服务
后台进程逐步将父Region的数据文件进行物理切分（Compaction）
分裂完成后，删除父Region的数据